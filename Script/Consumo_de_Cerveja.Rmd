---
title: "Consumo de Cerveja"
author: "Alex de Sousa Pereira"
date: "21/04/2021"
output: html_document
---
  (esse material foi 100% produzido usando a linguagem R)


# OBJETIVO DO TRABALHO

Este trabalho tem como objetivo  explorar dados do consumo de cerveja usando a biblioteca Rmarkdown, por meio de:

- Transformações dos dados;
- Análise descritiva dos dados;
- Análise multivariada dos dados.

## PARTE 01 - TRANSFORMAÇÕES DOS DADOS

### BIBLIOTECAS A SEREM UTILIZADAS

As bibliotecas que utilizaremos para este trabalho encontram-se abaixo: 

```{r,message=FALSE,warning=F}
# Carregando os pacotes 

library(dplyr) # Manipulação de dados
library(stringr) # manipulação de strings
library(lubridate) # manipulação de datas


```

### CARREGANDO OS DADOS

O arquivo dos nossos dados corresponde a um arquivo csv. Para lermos este arquivo, utilizaremos a funçao "read_csv" que já é vinculada ao R. 

Logo após, analisaremos a dimensão de nossos dados através da função "dim", e utilizaremos a função "head" afim de analisar as 5 primeiras linhas de nosso dataset.

```{r,message=FALSE,warning=F}
options(knitr.duplicate.label = "allow")
df = read.csv("https://raw.githubusercontent.com/alexdesousapereira/Analise-do-Consumo-de-Bebida-Alcoolica/main/Dados/consumo_cerveja.csv",sep = ";")
dim(df)
head(df)

```

### DESCRIÇÃO DAS COLUNAS

Descrevendo as colunas:

Data: dia que foram coletados os dados.

Temperatura média (C): temperatura média para o respectivo dia.

Temperatura mínima (C): temperatura mínima para o respectivo dia.

Temperatura máxima (C): temperatura máxima para o respectivo dia.

Precipitação (mm): quantidade em mililitros de chuva para o respectivo dia.

Final de Semana: se o respectivo dia era final de semana.

(1=sim, 0=não).

Consumo de cerveja (litros): consumo de cerveja em litros para o respectivo dia.

Observando o resultado acima, verificamos a quantidade total de registros com 365 linhas e 7 colunas.

### LIMPEZA E TRANSFORMAÇÃO DAS VARIÁVEIS

Primeiramente iremos analisar o tipo de nossas variáveis.

```{r,message=FALSE,warning=F}
str(df)

```
Transformaremos primeiramente o período dos dados, que foi designado como sendo de 2019, mas pertence ao ano de 2015.
```{r,message=FALSE,warning=F}
data_cor <- sub('2019','2015',df$Data)
data_cor <- sub('/','-',data_cor)
data_cor <- sub('/','-',data_cor)
df$Data <- data_cor
```


Logo, verificamos que algumas variáveis estão em formato caractere. Então, iremos fazer a transformarção destas variáveis.

```{r,message=FALSE,warning=F}
df$Temperatura.Maxima..C. =  df$Temperatura.Maxima..C. %>% str_replace('[,]', '.')%>% as.double()
df$Temperatura.Media..C. = df$Temperatura.Media..C. %>% str_replace('[,]', '.') %>% as.double()
df$Temperatura.Minima..C. = df$Temperatura.Minima..C. %>% str_replace('[,]', '.') %>% as.double()
df$Precipitacao..mm. = df$Precipitacao..mm. %>% str_replace('[,]', '.') %>% as.double()

```

Feito a transformação destas colunas, iremos verificar a presença de valores nulos em nosso dataset.

```{r,message=FALSE,warning=F}
df[!complete.cases(df),] # Verifica valores nulos no dataset
```

Como os valores que estão em nossas colunas podem ser relevantes para nossa análise, não iremos remover as linhas que contêm valores nulos.

Então, iremos substituir estes valores pela mediana de cada variável que se encontra com valor nulo. Outras soluções possiveís, seriam utilizar-mos a média, ou um modelo de machine learning para tentar prever esses valores para cada variável.

```{r,message=FALSE,warning=F}
df[is.na(df$Temperatura.Maxima..C.),]$Temperatura.Maxima..C. = median(df$Temperatura.Maxima..C.,na.rm = T)
df[is.na(df$Temperatura.Minima..C.),]$Temperatura.Minima..C. = median(df$Temperatura.Minima..C.,na.rm = T)
df[is.na(df$Temperatura.Media..C.),]$Temperatura.Media..C. = median(df$Temperatura.Media..C.,na.rm = T)
df[!complete.cases(df),] # Verificando se ainda existem valores nulos no dataset
```

Agora, iremos fazer uma transformação afim de facilitar em nossa análise de dados. Para tanto, iremos transformar a coluna "Final de Semana", em que:

0 = "Dia da Semana";

1 = "Final de Semana".

```{r,message=FALSE,warning=F}
#Convertendo coluna Final de Semana
# Vamos Converter algumas variáveis agora
df$Final.de.Semana = cut(df$Final.de.Semana, 
                         c(-1,0,1), 
                         labels = c("Dia de Semana","Fim de Semana"))
```

### CRIANDO COLUNA DIA E MÊS

Utilizare-mos o pacote "lubridate", para criar uma coluna "dia da semana" e uma coluna "mês" para ajudar na análise dos dados.

```{r,message=FALSE,warning=F}
# Criando coluna Dia da Semana
df$Dia.da.Semana = c(seq(1:length(df$Data)))
for (i in 1:length(df$Dia.da.Semana)){
  df$Dia.da.Semana[i] = wday(df$Data[i]) 
}
df$Dia.da.Semana= cut(df$Dia.da.Semana, 
                         c(0,1,2,3,4,5,6,7), 
                         labels = c("Domingo",
                                    "Segunda",
                                    "Terça",
                                    "Quarta",
                                    "Quinta",
                                    "Sexta",
                                    "Sábado"))
# Criando coluna Mês 
df$Mes = c(seq(1:length(df$Data)))
for (i in 1:length(df$Data)){
  df$Mes[i] = month(df$Data[i])
}
df$Mes= cut(df$Mes, 
              c(0,1,2,3,4,5,6,7,8,9,10,11,12), 
              labels = c("Janeiro",
                         "Fevereiro",
                         "Março",
                         "Abril",
                         "Maio",
                         "Junho",
                         "Julho",
                         "Agosto",
                         "Setembro",
                         "Outubro",
                         "Novembro",
                         "Dezembro"))
```


## PARTE 2 - ANÁLISE DESCRITIVA DOS DADOS
Nesta parte, iremos analisar nossos dados graficamente afim de chegar a possivéis conclusões.

### BIBLIOTECAS A SEREM UTILIZADAS

As bibliotecas que utilizaremos para este trabalho encontram-se abaixo: 


```{r,message=FALSE,warning=F}
# Carregando os pacotes 
library(ggplot2) # Criação de gráficos
library(ggcorrplot) # criação do gráfico de correlação

```

### ANÁLISE DE CORRELAÇÃO

Nesta etapa, iremos analisar quais variáveis são mais correlacionadas com o consumo de cerveja.

```{r,message=FALSE,warning=F}
df_corr = select_if(df,is.numeric) %>% cor() %>% round(2)
ggcorrplot(df_corr,type = "lower", # Retira o espelho da matriz
           hc.order = T, # organiza os valores 
           method = "circle", # formato dos componentes da matriz
           lab = TRUE, # adicionar os valores das correlações
           lab_size = 2,# Tamamho dos Valores
           colors = c("firebrick", "white","blue"), # cores do gráfico
           title = "Matriz de Correlação"
           ) 
```

Analisando o gráfico de correlação, pode-se observar que existe uma correlação negativa de ‘consumo’ com "precipitação",ou seja,a medida que chove mais o consumo de cerveja diminui.

Pode-se observar também, que existe uma correlação positiva de "consumo" com "temperatura máxima", ou seja, quanto maior a temperatura, maior será o consumo de cerveja.

### CONSUMO DE CERVEJA POR DIA DA SEMANA

Este tópico tem como objetivo analisar o consumo de bebida alcoólica por dia da semana.

```{r,message=FALSE,warning=F}
ggplot(df) +  
  aes(x = Dia.da.Semana,y = Consumo.de.cerveja..litros.,col = Dia.da.Semana) +
  labs(x = "Dia da Semana",
       y = "Consumo de Cerveja",
       title = "Consumo de Cerveja na Semana") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6),
        # Centralizar o título
        plot.title = element_text(hjust = 0.5),
        #Tirar o título do eixo x
        axis.title.x=element_blank(),
        #Tirar o título do eixo y
        axis.title.y=element_blank()) +
  geom_boxplot()

```

Observando o gráfico, pode-se notar que temos um maior consumo de cerveja, consequentemente na segunda, quarta e sexta - feira. Em que, somente na quinta - feira, tivemos valores fora da média.

### CONSUMO DE CERVEJA NO MÊS

Este tópico tem como objetivo analisar o consumo de bebida alcoólica por mês.

```{r,message=FALSE,warning=F}
# Consumo de Cerveja por mês  

ggplot(df) +  
  aes(x = Mes,y = Consumo.de.cerveja..litros.) +
  labs(x = "Mês",
       y = "Consumo de Cerveja",
       title = "Consumo de Cerveja no Mês") +
  geom_col(stat="identity", width = 0.5, fill="tomato2") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6),
        # Centralizar o título
        plot.title = element_text(hjust = 0.5),
        #Tirar o título do eixo x
        axis.title.x=element_blank(),
        #Tirar o título do eixo y
        axis.title.y=element_blank()) +
  ylim(limits= c(0,100000)) + 
  coord_flip()

```

Observa-se que o mês de outubro, foi aquele que tivemos o maior consumo de cerveja e, consequetemente o mês abril foi o período com menor consumo.

### CONSUMO MÉDIO DE CERVEJA NO FINAL DE SEMANA

Este tópico têm como principal objetivo analisar o consumo de cerveja nos finais de semana.

```{r,message=FALSE,warning=F}
# Consumo médio de cerveja no Dia de Semana e no Final de Semana
X = group_by(df,Final.de.Semana)%>%summarise(Total=mean(Consumo.de.cerveja..litros.))
ggplot(X) +  
  aes(x =Final.de.Semana ,y = Total) +
  labs(title = "Consumo Médio de Cerveja") +
  geom_col(width= 0.2,size = 1,color="darkblue",fill="white") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6),
        #Centralizar o título
        plot.title = element_text(hjust = 0.5),
        #Tirar o título do eixo x
        axis.title.x=element_blank(),
        # Tirar o título do eixo y
        axis.title.y=element_blank()
  )

```

Analisando, pode-se observar que temos um consumo maior de cerveja nos fins de semana do que nos dias de semana.


<<<<<<< HEAD
## PARTE 03 - ANÁLISE MULTIVARIADA
  
  Neste etapa do projeto, iremos utilizaremos da análise multivariada com a finalidade de prever o consumo de cerveja.
  
### BIBLIOTECAS A SEREM UTILIZADAS

As bibliotecas que utilizaremos para este trabalho encontram-se abaixo:

```{r,message=FALSE,warning=F}

library(MASS) #Pacote para a Análise de Regressão (modelagem linear)
library(lmtest) #Pacote de Testes
library(car)#Pacote realizar teste de multicolinearidade
library(forecast)#Pacote para utilizar função Box.Cox

```

### REDUZINDO O NOME DAS VARIÁVEIS
Antes de fazermos a análise preditiva, iremos reduzir o nome das váriaveis com o objetivo de facilitar em nossa análise.
=======

  
  
>>>>>>> 6fcccb88a01e3fa16175a65323d0d04881f09f58

```{r,message=FALSE,warning=F}
df <- df %>% dplyr::rename(
    dia = Data,
    temp_med = `Temperatura.Media..C.`,
    temp_min = `Temperatura.Minima..C.`,
    temp_max = `Temperatura.Maxima..C.`,
    precip = `Precipitacao..mm.`,
    fds = `Final.de.Semana`,
    consumo = `Consumo.de.cerveja..litros.`
  )
```

### ENCONTRANDO O MELHOR MODELO 
  Agora, iremos nossas análise para tentar encontrar o melhor modelo preditivo. 
```{r,message=FALSE,warning=F}

modelo1=lm(consumo~ temp_med+temp_min+temp_max+precip+fds,
          data = df)
summary(modelo1)
```
Note que as variaveis temperatura mínima e temperatura média não se demonstraram significativas ao nível de 5% de significância. Deste modo, iremos removê-las do nosso modelo!
```{r,message=FALSE,warning=F}

modelo2=lm(consumo~temp_max+precip+fds,
          data = df)
summary(modelo2)
```

### ANÁLISE DE RESÍDUOS
Neste etapa, iremos análisar os presuposto de um modelo de regressão linear. Sendo eles:

1-) Normalidade;
2-) Homocedasticidade;
3-) Outliers;
4-) Indepedência de residuos;
5-) Multicolineariedade.

```{r,message=FALSE,warning=F}
#Análise Grafica de Residuos

par(mfrow=c(2,2))
plot(modelo2,which = c(1:4),pch=20)

```
O primeiro gráfico, nos demostra os resíduos pelos valores ajustados, a partir dele pode-mos ver a lineriadade de nossos dados, logo que a linha vermelha se encontra aproximadamente horizontal.

O segundo, nos demostra que resíduos não seguem uma distribuição normal, logo que os resíduos não se encontram próximos da linha pontilhada. 

O terceiro gráfico nos demostra que os resíduos são homocedásticos, logo que a maioria dos residuos seguem um padrão retangular.


O Quarto gráfico nós demostra que não existem a presença de outliers em nossos dados, logo que as observações tem uma cook's distance < 1 .

Agora, iremos analisar teste por teste a fim de confirmar nossas suposições visuais.

1-) Normalidade -> a partir do shapiro.test podemos verificar se os dados seguem uma normal padrão.

```{r,message=FALSE,warning=F}
#Análise de Normalidade

shapiro.test(modelo2$residuals)

```
Os resíduos não seguem uma normal padrão, logo que o p-valor < 0,05.

2-) Outliers nos resíduos-> por meio do código abaixo é possível identificar se existe outliers nos resíduos

```{r,message=FALSE,warning=F}
summary(rstandard(modelo2))

```
Não existem outliers nos resíduos.

3-) Teste de Homocedasticidade -> por meio do código abaixo é possível identificar se existe homocedasticidade.

```{r,message=FALSE,warning=F}
bptest(modelo2)

```
Observa-se que não há homocedasticidade. No entanto, nosso teste não é válido, logo que a nossa distribuição não segue uma normal padrão.
4-) Teste de Indepedência de Resíduos
```{r,message=FALSE,warning=F}
durbinWatsonTest(modelo2)

```
Pelo teste observa-se que não existe indepedência de resíduos, logo que teste de DurbinWatson encontra-se entre 1,5 e 2 e o p-value>0,05. No entanto, nosso teste não é valido, pois nossa distribuição não segue uma normal padrão.


5-) Analisando a multicolienariedade -> por meio do código abaixo é possível identificar se existe multicolienariedade.

```{r,message=FALSE,warning=F}
vif(modelo2)

```
Não existe multicolinariedade entre as váriveis independentes, logo que o VIF < 10.


### TRANSFORMAÇÃO DO MODELO
Agora iremos transformar nosso modelo de regressão com a finalidade de deixa-lo ajustado para que sega uma distribuição normal. A partir da transformação Box-cox é possível encontrar a melhor transformação para o modelo.

```{r,message=FALSE,warning=F}
lambda<- BoxCox.lambda(modelo2$residuals)
lambda

Y <- BoxCox(df$consumo, lambda)
```

Feito a transformação, iremos carregar novamente o modelo ajustado.

```{r,message=FALSE,warning=F}

modelo3=lm(Y~temp_max+precip+fds,
          data = df)
summary(modelo3)


```

Agora fazeremos novamente a análise dos resíduos com o modelo ajustado.

```{r,message=FALSE,warning=F}
#Análise de Normalidade

shapiro.test(modelo3$residuals)

```

